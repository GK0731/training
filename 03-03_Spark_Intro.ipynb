{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark is easy to program and don't require that much hand coding whereas MapReduce is not that easy in terms of programming and requires lots of hand coding\n",
    "2. It has interactive mode whereas in MapReduce there is no built-in interactive mode, MapReduce is developed for batch processing.\n",
    "3. For data processing Spark can use streaming, machine learning, and batch processing whereas Hadoop MapReduce can use the batch engine. Spark is general purpose cluster computation engine.\n",
    "4. Spark executes batch processing jobs about 10 to 100 times faster than Hadoop MapReduce.\n",
    "5. Spark uses an abstraction called RDD which makes Spark feature rich, whereas map reduce doesn't have any abstraction\n",
    "6. Spark uses lower latency by caching partial/complete results across distributed nodes whereas MapReduce is completely disk-based.\n",
    "\n",
    "Source: https://community.hortonworks.com/questions/129786/benefits-of-spark-over-mapreduce-or-spark-vs-mapre.html\n",
    "\n",
    "Detailed Comparison: https://data-flair.training/blogs/apache-spark-vs-hadoop-mapreduce/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RDD_DATAFRAME_DATASETS.png\" width=\"400\" height=\"auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RDD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\n",
    "\n",
    "- Spark RDD APIs – An RDD stands for Resilient Distributed Datasets. It is __Read-only__ partition collection of records. RDD is the fundamental data structure of Spark. It allows a programmer to perform in-memory computations on large clusters in a fault-tolerant manner. Thus, speed up the task. Follow [this link to learn Spark RDD in great detail](https://data-flair.training/blogs/apache-spark-rdd-tutorial/).\n",
    "- Spark Dataframe APIs – Unlike an RDD, data organized into __named columns__. For example a table in a relational database. It is an immutable distributed collection of data. DataFrame in Spark allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction. Follow [this link to learn Spark DataFrame in detail](http://data-flair.training/blogs/apache-spark-sql-dataframe-tutorial/).\n",
    "- Spark Dataset APIs – Datasets in Apache Spark are an __extension__ of DataFrame API which provides type-safe, object-oriented programming interface. Dataset takes advantage of Spark’s Catalyst optimizer by exposing expressions and data fields to a query planner. Follow [this link to learn Spark DataSet in detail](http://data-flair.training/blogs/apache-spark-dataset-tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Data Representation\n",
    "- RDD – RDD is a distributed collection of data elements spread across many machines in the cluster. RDDs are a set of Java or Scala objects representing data.\n",
    "- DataFrame – A DataFrame is a distributed collection of data organized into named columns. It is conceptually equal to a table in a relational database.\n",
    "- DataSet – It is an extension of DataFrame API that provides the functionality of – type-safe, object-oriented programming interface of the RDD API and performance benefits of the Catalyst query optimizer and off heap storage mechanism of a DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Data Formats\n",
    "- RDD – It can easily and efficiently process data which is structured as well as unstructured. But like Dataframe and DataSets, RDD does not infer the schema of the ingested data and requires the user to specify it.\n",
    "- DataFrame – It can process structured and unstructured data efficiently. It organizes the data in the named column. DataFrames allow the Spark to manage schema.\n",
    "- DataSet – It also efficiently processes structured and unstructured data. It represents data in the form of JVM objects of row or a collection of row object. Which is represented in tabular forms through encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4. Data Sources API\n",
    "- RDD – Data source API allows that an RDD could come from __any data source__ e.g. text file, a database via JDBC etc. and easily handle data with __no predefined structure__.\n",
    "- DataFrame – Data source API allows Data processing in different formats (AVRO, CSV, JSON, and storage system HDFS, HIVE tables, MySQL). It can read and write from various data sources that are mentioned above.\n",
    "- DataSet – Dataset API of spark also support data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5. Immutability and Interoperability\n",
    "- RDD – RDDs contains the collection of records which are partitioned. The basic unit of parallelism in an RDD is called partition. Each partition is one logical division of data which is __immutable__ and created through some transformation on existing partitions. Immutability helps to achieve consistency in computations. We can move from RDD to DataFrame (If RDD is in tabular format) by toDF() method or we can do the reverse by the .rdd method. Learn various RDD Transformations and Actions APIs with examples.\n",
    "- DataFrame – After transforming into DataFrame one cannot regenerate a domain object. For example, if you generate testDF from testRDD, then you won’t be able to recover the original RDD of the test class.\n",
    "- DataSet – It overcomes the limitation of DataFrame to regenerate the RDD from Dataframe. Datasets allow you to convert your existing RDD and DataFrames into Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.7. Optimization\n",
    "- RDD – No inbuilt optimization engine is available in RDD. When working with structured data, RDDs cannot take advantages of sparks advance optimizers. For example, catalyst optimizer and Tungsten execution engine. Developers optimise each RDD on the basis of its attributes.\n",
    "- DataFrame – Optimization takes place using catalyst optimizer. Dataframes use catalyst tree transformation framework in four phases: a) Analyzing a logical plan to resolve references. b) Logical plan optimization. c) Physical planning. d) Code generation to compile parts of the query to Java bytecode. The brief over.jpgiew of optimization phase is also given in the below figure:\n",
    "<br>\n",
    "<img src=\"img/Spark-SQL-Optimization.jpg\" height=\"auto\" width=\"800\">\n",
    "<br>\n",
    "- Dataset – It includes the concept of Dataframe Catalyst optimizer for optimizing query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.8. Serialization\n",
    "- RDD – Whenever Spark needs to distribute the data within the cluster or write the data to disk, it does so use Java serialization. The overhead of serializing individual Java and Scala objects is expensive and requires sending both data and structure between nodes.\n",
    "- DataFrame – Spark DataFrame Can serialize the data into off-heap storage (in memory) in binary format and then perform many transformations directly on this off heap memory because spark understands the schema. There is no need to use java serialization to encode the data. It provides a Tungsten physical execution backend which explicitly manages memory and dynamically generates bytecode for expression evaluation.\n",
    "- DataSet – When it comes to serializing data, the Dataset API in Spark has the concept of an encoder which handles conversion between JVM objects to tabular representation. It stores tabular representation using spark internal Tungsten binary format. Dataset allows performing the operation on serialized data and improving memory use. It allows on-demand access to individual attribute without desterilizing the entire object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.10. Efficiency/Memory use\n",
    "- RDD – Efficiency is decreased when serialization is performed individually on a java and scala object which takes lots of time.\n",
    "- DataFrame – Use of off heap memory for serialization reduces the overhead. It generates byte code dynamically so that many operations can be performed on that serialized data. No need for deserialization for small operations.\n",
    "- DataSet – It allows performing an operation on serialized data and improving memory use. Thus it allows on-demand access to individual attribute without deserializing the entire object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.14. Aggregation\n",
    "- RDD – RDD API is slower to perform simple grouping and aggregation operations.\n",
    "- DataFrame – DataFrame API is very easy to use. It is faster for exploratory analysis, creating aggregated statistics on large data sets.\n",
    "- DataSet – In Dataset it is faster to perform aggregation operation on plenty of data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.15. Usage Area\n",
    "- RDD-You can use RDDs When you want low-level transformation and actions on your data set.Use RDDs When you need high-level abstractions.\n",
    "\n",
    "- DataFrame and DataSet- One can use both DataFrame and dataset API when we need a high level of abstraction. For unstructured data, such as media streams or streams of text. You can use both Data Frames or Dataset when you need domain specific APIs. When you want to manipulate your data with functional programming constructs than domain specific expression. We can use either datasets or DataFrame in the high-level expression. For example, filter, maps, aggregation, sum, SQL queries, and columnar access. When you do not care about imposing a schema, such as columnar format while processing or accessing data attributes by name or column. in addition, If we want a higher degree of type safety at compile time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Action and Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/CORE_ACTIONS_TRANSFORMATIONS.png\" width=\"800\" height=\"auto\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark Transformation is a function that produces __new RDD__ from the __existing RDDs__.\n",
    "- Actions are Spark RDD operations that give __non-RDD values__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a file\n",
    "```scala\n",
    "sc.wholeTextFile()\n",
    "sc.textFile()\n",
    "```\n",
    "\n",
    "#### From a Data Source(sql, nosql)\n",
    "```scala\n",
    "spark.sql\n",
    "```\n",
    "\n",
    "#### In code\n",
    "```scala\n",
    "sc.parallelize()\n",
    "```\n",
    "\n",
    "\\* spark streaming is also a way to create rdd but is not included in the this training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA FROM FILE AND LOAD INTO DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "sc.wholeTextFile()\n",
    "sc.textFile()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml\n",
      "Checking https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml.sha1\n",
      "Checked https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml\n",
      "Checked https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml.sha1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` \n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "sc.wholeTextFile()\n",
    "sc.textFile()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@10b50d46"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  .config(\"spark.executor.instances\", \"2\")\n",
    "  .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@2db45edd"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLotInfo\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class LotInfo(TOOL: String, STEP: String, MATERIAL: String, V1: Double, V2: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"data/dataset_1.csv\")\n",
    "                    .map(line => line.split (\",\"))\n",
    "                    .map(p => LotInfo(p(0).toString, p(1).toString, p(2).toString, p(3).toDouble, p(4).toInt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [TOOL: string, STEP: string ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "        .format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"true\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"data/dataset_2.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|\n",
      "+------+------+----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  TOOL|\n",
      "+------+\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"TOOL\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|\n",
      "|tool 1|step 1|material 1|0.116683078|    914|\n",
      "|tool 1|step 1|material 2|0.830316155|    783|\n",
      "|tool 1|step 1|material 2|0.296609434|    261|\n",
      "|tool 1|step 1|material 2|0.381470171|    139|\n",
      "|tool 1|step 1|material 2|0.285614758|    663|\n",
      "+------+------+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"TOOL == 'tool 1'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.116683078|    914|\n",
      "|tool 2|step 2|material 3|0.359787374|    991|\n",
      "|tool 3|step 2|material 2|0.230862465|    912|\n",
      "|tool 3|step 2|material 3|0.765056471|    949|\n",
      "+------+------+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"VALUE_2 > 900\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA FROM HIVE AND LOAD INTO DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import spark.sql\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "val hql2: String =\"\"\"\n",
    "select\n",
    "   sample_date,\n",
    "   sample_id,\n",
    "   ch_id,\n",
    "   ckc_id,\n",
    "   max(sys_tt_upd_ts) as sys_tt_upd_ts,\n",
    "   avg(ext_ewma_mv) as ext_ewma_mv,\n",
    "   avg(ext_ewma_mv_ucl) as ext_ewma_mv_ucl,\n",
    "   avg(ext_ewma_mv_lcl) as ext_ewma_mv_lcl,\n",
    "   avg(ext_sigma_ucl) as ext_sigma_ucl,\n",
    "   avg(ext_sigma_lcl) as ext_sigma_lcl,\n",
    "   avg(ext_range_ucl) as ext_range_ucl,\n",
    "   avg(ext_range_lcl) as ext_range_lcl,\n",
    "   avg(ext_mv_ucl) as ext_mv_ucl,\n",
    "   avg(ext_mv_lcl) as ext_mv_lcl\n",
    "from prod_mti_singapore_fab_10_spc_dm.samples_calc\n",
    "where sys_part_yyyy = 2018 and sys_part_mm = 06\n",
    "group by sample_date, sample_id, ch_id, ckc_id\"\"\"\n",
    "\n",
    "val df2 = sql(hql); \n",
    "df2.show(10)    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/df-show.png\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE DATA DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "df2\n",
    ".repartition(1)\n",
    ".write\n",
    ".format(\"csv\")\n",
    ".option(\"sep\", \"\\t\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"header\", \"false\")\n",
    ".save(\"/eng/mti/singapore/fab_10/qm69a_v2/input_data/space/charts_daily/query_date=\" + \"20180619\")\n",
    "\n",
    "sql(\"ALTER TABLE eng_mti_singapore_fab_10_autodiagnostic_v2.space_charts_daily ADD IF NOT EXISTS partition (query_date=\\\"\"+ \"20180619\" +\"\\\") location \\\"/eng/mti/singapore/fab_10/qm69a_v2/input_data/space/charts_daily/query_date=\"+ '20180619' +\"\\\"\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA FROM HBASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.hadoop.hbase.client.Connection\n",
    "import org.apache.hadoop.hbase.client.ConnectionFactory\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "import org.apache.hadoop.hbase.{CellUtil, HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.client._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "def printRow(result : Result) = {\n",
    "val cells = result.rawCells();\n",
    "print( Bytes.toString(result.getRow) + \" : \" )\n",
    "for(cell <- cells){\n",
    "  val col_name = Bytes.toString(CellUtil.cloneQualifier(cell))\n",
    "  val col_value = Bytes.toString(CellUtil.cloneValue(cell))\n",
    "  print(\"(%s,%s) \".format(col_name, col_value))\n",
    "}\n",
    "println()\n",
    "}\n",
    "\n",
    "object SharedHBaseConnection extends Serializable{\n",
    "  private var sharedConnection: Option[Connection] = None\n",
    "\n",
    "  lazy val config = {\n",
    "    val config = HBaseConfiguration.create()\n",
    "    config.addResource(new Path(\"file:///usr/hdp/current/hbase-client/conf/hbase-site.xml\"))\n",
    "    config\n",
    "  }\n",
    "\n",
    "  def apply(): Connection = synchronized {\n",
    "    sharedConnection.getOrElse {\n",
    "      val connection = ConnectionFactory.createConnection(config)\n",
    "      sharedConnection = Some(connection)\n",
    "      connection\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "val hbaseConnection = SharedHBaseConnection()\n",
    "val dataTable = hbaseConnection.getTable(TableName.valueOf(\"prod_mti_singapore_fab_10_sigma:sigma_wafer\"))\n",
    "println(\"connection created\")\n",
    "\n",
    "println(\"Get Example:\")\n",
    "var get = new Get(Bytes.toBytes(\"1707981_0711-01\"))\n",
    "var result = dataTable.get(get)\n",
    "printRow(result)\n",
    "\n",
    "dataTable.close()\n",
    "hbaseConnection.close()\n",
    "\n",
    "System.exit(0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATA FROM HDFS AND DO WORD COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "val sqlText = sc.textfile(\"/user/hdfsf10n/pengtan/spark_hive_test/query_date=20180513/part-00000-e7587264-467d-46c0-bfdc-2364b9a09d6e.csv.deflate\");\n",
    "\n",
    "val count = sqlText\n",
    "           .flatMap(line => line.split(\" \"))\n",
    "           .map(word => (word,1))\n",
    "           .reduceByKey(_+_)\n",
    "           .collect()\n",
    "count.foreach(x => println(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between spark-submit and spark-shell\n",
    "\n",
    "- spark-shell will initial sparkSession itself, avaialable as sc directly in the spark-shell\n",
    "- spark-submit you need to config sparkSession in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open a spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "export SPARK_MAJOR_VERSION=2\n",
    "spark-shell --master yarn --driver-memory 4g --executor-memory 2g --executor-cores 2 --driver-cores 1 --queue eng_f10w-01 --name spark_test_f10ds_pengtan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tricks when using spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "\n",
    "```\n",
    "sc (sparkContext) and spark (sparkSession) will auto generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    ":paste\n",
    "```\n",
    "use :paste to paste large block of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful to use __tab__ (try to avoid them by replace them with 4 space instead)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
