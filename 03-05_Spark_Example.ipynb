{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Spark in Jupyter (BeakerX) \n",
    "> Use this if you are using BeakerX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-sql_2.11 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark\n",
    "\n",
    "[Spark Examples](https://spark.apache.org/examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark Tutorial](http://backtobazics.com/tutorials/scala-tutorial-for-java-programmers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val conf = new SparkConf()\n",
    "  .setMaster(\"local[*]\")\n",
    "  .set(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark Demo\")\n",
    "  .config(conf)\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark` and `sc` are automatically generated in spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at map at <console>:16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class LotInfo(TOOL: String, STEP: String, MATERIAL: String, V1: Double, V2: Int)\n",
    "val textFile = sc.textFile(\"data/dataset_1.csv\")\n",
    "                .map(line => line.split (\",\"))\n",
    "                .map(p => LotInfo(p(0).toString, p(1).toString, p(2).toString, p(3).toDouble, p(4).toInt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LotInfo(ï»¿tool 1,step 1,material 1,0.869495157,318), LotInfo(tool 1,step 1,material 1,0.357759255,579), LotInfo(tool 1,step 1,material 1,0.255350443,188), LotInfo(tool 1,step 1,material 1,0.774128219,512), LotInfo(tool 1,step 1,material 1,0.272550202,110)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b65069-01b5-487c-b1c0-e4d9195d6a65",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "        .format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"true\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"data/dataset_2.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TOOL: string (nullable = true)\n",
      " |-- STEP: string (nullable = true)\n",
      " |-- MATERIAL: string (nullable = true)\n",
      " |-- VALUE_1: string (nullable = true)\n",
      " |-- VALUE_2: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|\n",
      "+------+------+----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read DF from Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "val hql: String =\"\"\"\n",
    "select\n",
    "   sample_date,\n",
    "   sample_id,\n",
    "   ch_id,\n",
    "   ckc_id\n",
    "from prod_mti_singapore_fab_10_spc_dm.samples_calc\n",
    "where sys_part_yyyy = 2018 and sys_part_mm = 06\n",
    "\"\"\"\n",
    "\n",
    "val df = spark.sql(hql); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|\n",
      "|tool 1|step 1|material 1|0.116683078|    914|\n",
      "|tool 1|step 1|material 2|0.830316155|    783|\n",
      "|tool 1|step 1|material 2|0.296609434|    261|\n",
      "|tool 1|step 1|material 2|0.381470171|    139|\n",
      "|tool 1|step 1|material 2|0.285614758|    663|\n",
      "|tool 2|step 1|material 2|0.397976087|    880|\n",
      "|tool 2|step 1|material 2|0.134806317|    558|\n",
      "|tool 2|step 1|material 3|0.602803637|    570|\n",
      "|tool 2|step 1|material 3|0.080131348|    891|\n",
      "|tool 2|step 1|material 3|0.228636809|    657|\n",
      "|tool 2|step 2|material 3|0.688289551|    788|\n",
      "|tool 2|step 2|material 3|0.359787374|    991|\n",
      "|tool 2|step 2|material 3| 0.21757853|    144|\n",
      "|tool 2|step 2|material 2|0.713266982|    200|\n",
      "|tool 2|step 2|material 2|0.817362621|    114|\n",
      "|tool 3|step 2|material 2|0.665453112|    432|\n",
      "|tool 3|step 2|material 2|0.234020735|    500|\n",
      "|tool 3|step 2|material 3|0.901529745|     60|\n",
      "|tool 3|step 2|material 3|0.647947866|    232|\n",
      "|tool 3|step 2|material 3|0.413327385|    799|\n",
      "|tool 3|step 2|material 1|0.510226254|     78|\n",
      "|tool 3|step 2|material 2|0.368676436|    155|\n",
      "|tool 3|step 2|material 2|0.230862465|    912|\n",
      "|tool 3|step 2|material 3|0.263936235|    200|\n",
      "|tool 3|step 2|material 3|0.765056471|    949|\n",
      "+------+------+----------+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|\n",
      "+------+------+----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, lit}\n",
    "df.filter(df(\"TOOL\") === lit(\"tool 1\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+------+----------+-----------+-------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|\n",
      "+------+------+----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"TOOL = 'tool 1'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- withColumn\n",
    "\n",
    "> withColumn(<new_column>, {expression_to_build_new_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+-------+------------------+\n",
      "|  TOOL|  STEP|  MATERIAL|    VALUE_1|VALUE_2|           NEW_COL|\n",
      "+------+------+----------+-----------+-------+------------------+\n",
      "|tool 1|step 1|material 1|0.715171243|    986|       2.145513729|\n",
      "|tool 1|step 1|material 1|0.958547461|     35|2.8756423829999997|\n",
      "|tool 1|step 1|material 1|0.542267277|    589|1.6268018309999999|\n",
      "|tool 1|step 1|material 1|0.778020313|    967|       2.334060939|\n",
      "|tool 1|step 1|material 1|0.820385525|    500|       2.461156575|\n",
      "+------+------+----------+-----------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"NEW_COL\", df(\"VALUE_1\") * 3).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  TOOL|\n",
      "+------+\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "|tool 1|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"TOOL\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- toPandas()         \n",
    "> pyspark only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+-------+\n",
      "|  STEP|  MATERIAL|    VALUE_1|VALUE_2|\n",
      "+------+----------+-----------+-------+\n",
      "|step 1|material 1|0.715171243|    986|\n",
      "|step 1|material 1|0.958547461|     35|\n",
      "|step 1|material 1|0.542267277|    589|\n",
      "|step 1|material 1|0.778020313|    967|\n",
      "|step 1|material 1|0.820385525|    500|\n",
      "+------+----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"TOOL\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/dataset_1.csv MapPartitionsRDD[74] at textFile at <console>:108"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = sc.textFile(\"data/dataset_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ï»¿tool 1,step 1,material 1,0.869495157,318]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[79] at map at <console>:108"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfRdd = tf.map(line => line.split (\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ï»¿tool 1, step 1, material 1, 0.869495157, 318], [tool 1, step 1, material 1, 0.357759255, 579]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfRdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[78] at flatMap at <console>:108"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfFmRdd = tf.flatMap(line => line.split (\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ï»¿tool 1, step 1]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = tfFmRdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tool 1, step 1, material 1, 0.357759255, 579], [tool 1, step 1, material 1, 0.255350443, 188], [tool 1, step 1, material 1, 0.774128219, 512], [tool 1, step 1, material 1, 0.272550202, 110], [tool 1, step 1, material 1, 0.327189103, 11]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.map(line => line.split (\",\")).filter(s => s(0) == \"tool 1\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/spark-reduceByKey.png\" width=\"600\" height=\"450\" style=\"display: block; margin: 0 auto\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(step 2,6.579661281000001), (step 1,7.814073281000001)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = sc.textFile(\"data/dataset_1.csv\")\n",
    "tf.map(line => line.split (\"\\t\"))\n",
    "    .map(s => (s(1).trim, s(3).toDouble))\n",
    "    .reduceByKey(_ + _)\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/spark-groupByKey.png\" width=\"600\" height=\"450\" style=\"display: block; margin: 0 auto\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tool 2,CompactBuffer(0.913789601, 0.450131428, 0.335269137, 0.240003101, 0.680100259, 0.177084315, 0.409285299, 0.467945493, 0.375077091, 0.536814527))\n",
      "(tool 3,CompactBuffer(0.479830267, 0.304870537, 0.522640386, 0.02014818, 0.279830528, 0.271758789, 0.436192934, 0.933186634, 0.73414128, 0.630855021))\n",
      "(tool 1,CompactBuffer(0.869495157, 0.357759255, 0.255350443, 0.774128219, 0.272550202, 0.327189103, 0.642169036, 0.651917744, 0.759575849, 0.284644747))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = sc.textFile(\"data/dataset_1.csv\")\n",
    "tf.map(line => line.split (\"\\t\"))\n",
    "    .map(s => (s(0).trim, s(3).toDouble))\n",
    "    .groupByKey().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- foreach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- saveAsTextFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default it will save to HDFS. use file:/// to save to local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.map(line => line.split (\"\\t\"))\n",
    "    .map(s => (s(0).trim, s(3).toDouble))\n",
    "    .groupByKey()\n",
    "    .repartition(1)\n",
    "    .saveAsTextFile(\"file:///data/sampleDataOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.write\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .save(\"file:///data/sampleDataDFOutput\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
